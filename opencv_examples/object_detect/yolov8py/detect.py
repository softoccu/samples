import cv2
import time
import os
import numpy as np
from datetime import datetime, timedelta
from multiprocessing import Process, Queue
from ultralytics import YOLO
import signal

# ---------------------------
# å‚æ•°é…ç½®
# ---------------------------
FRAME_WIDTH = 1024
FRAME_HEIGHT = 576
FPS = 15
SEGMENT_HOURS = 3
AREA_THRESHOLD = 0.02
SKIP_FRAMES = 15
CONFIDENCE_THRESHOLD = 0.3
EXPOSURE_RANGE = (-11, -2)
TARGET_BRIGHTNESS_RANGE = (60, 110)
FRAME_IMAGE_DIR = 'person_frames'
VIDEO_DIR = 'videos'

os.makedirs(FRAME_IMAGE_DIR, exist_ok=True)
os.makedirs(VIDEO_DIR, exist_ok=True)

# ---------------------------
# å­è¿›ç¨‹ï¼šå›¾åƒæ£€æµ‹é€»è¾‘
# ---------------------------
def detector_process(frame_queue: Queue):
    signal.signal(signal.SIGINT, signal.SIG_IGN)  # å¿½ç•¥ Ctrl+C ä¸­æ–­

    print("[å­è¿›ç¨‹] YOLO æ­£åœ¨åŠ è½½ä¸­...")
    model = YOLO('yolov8s.pt')  # åˆ‡æ¢ä¸º yolov8s æ¨¡å‹
    model.fuse()
    print("[å­è¿›ç¨‹] âœ… YOLO åŠ è½½å®Œæˆï¼Œç­‰å¾…å›¾åƒæ£€æµ‹")

    while True:
        item = frame_queue.get()
        if item == "QUIT":
            print("[å­è¿›ç¨‹] ğŸ”š æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œä¼˜é›…é€€å‡º")
            break

        frame, timestamp_str = item
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        height, width = gray.shape

        results = model.predict(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)
        detections = results[0].boxes

        for box in detections:
            cls_id = int(box.cls.item())
            conf_score = float(box.conf.item())
            if cls_id != 0:
                continue

            x1, y1, x2, y2 = map(int, box.xyxy[0])
            area = (x2 - x1) * (y2 - y1)
            area_ratio = area / (width * height)

            if area_ratio >= AREA_THRESHOLD:
                # ç”»ç»¿è‰²æ¡†å’Œç±»åˆ«æ–‡å­—
                label = results[0].names[cls_id] if hasattr(results[0], 'names') else 'person'
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)
                cv2.putText(frame, f"{label} {conf_score:.2f}", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

                img_name = f"person_{datetime.now().strftime('%Y%m%d_%I%M%S_%p')}.jpg"
                img_path = os.path.join(FRAME_IMAGE_DIR, img_name)
                cv2.imwrite(img_path, frame)
                print(f"[å­è¿›ç¨‹] ğŸ’¾ ä¿å­˜å›¾åƒï¼š{img_path}ï¼ˆé¢ç§¯æ¯”ä¾‹={area_ratio:.3f}, ç½®ä¿¡åº¦={conf_score:.2f}ï¼‰")
                break

# ---------------------------
# ä¸»è¿›ç¨‹ï¼šå½•åƒä¸æ›å…‰è°ƒèŠ‚
# ---------------------------
def recorder_main():
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, FPS)

    exposure_mode = "auto"
    current_exposure = -6
    last_exposure_update = datetime.min
    last_brightness = 0.0
    cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.75)
    print("ğŸ§ª å¯åŠ¨è‡ªåŠ¨æ›å…‰")

    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    def new_writer():
        tstr = datetime.now().strftime("%Y%m%d_%I%M%S_%p")
        path = os.path.join(VIDEO_DIR, f"record_{tstr}.avi")
        return cv2.VideoWriter(path, fourcc, FPS, (FRAME_WIDTH, FRAME_HEIGHT)), datetime.now(), path

    out, segment_start_time, current_video_path = new_writer()
    print("ğŸŸ¢ å¼€å§‹å½•åƒï¼š", current_video_path)

    frame_queue = Queue()
    detector = Process(target=detector_process, args=(frame_queue,))
    detector.start()

    frame_count = 0

    def adjust_manual_exposure():
        nonlocal current_exposure, last_brightness
        for attempt in range(10):
            cap.set(cv2.CAP_PROP_EXPOSURE, current_exposure)
            time.sleep(0.2)
            ret, temp_frame = cap.read()
            if not ret:
                continue
            avg_brightness = np.mean(cv2.cvtColor(temp_frame, cv2.COLOR_BGR2GRAY))
            last_brightness = avg_brightness

            print(f"ğŸ”§ æ›å…‰å°è¯• {attempt+1}: EXP={current_exposure}, äº®åº¦={avg_brightness:.1f}, æ—¶é—´={datetime.now().strftime('%Y-%m-%d %I:%M:%S %p')}")

            if TARGET_BRIGHTNESS_RANGE[0] <= avg_brightness <= TARGET_BRIGHTNESS_RANGE[1]:
                print(f"âœ… æ›å…‰è°ƒæ•´æˆåŠŸï¼šEXP={current_exposure}, äº®åº¦={avg_brightness:.1f}")
                return

            if avg_brightness < TARGET_BRIGHTNESS_RANGE[0] and current_exposure < EXPOSURE_RANGE[1]:
                current_exposure += 1
            elif avg_brightness > TARGET_BRIGHTNESS_RANGE[1] and current_exposure > EXPOSURE_RANGE[0]:
                current_exposure -= 1
            else:
                break

        print(f"âš ï¸ æ›å…‰è°ƒæ•´å¤±è´¥ï¼Œä½¿ç”¨ EXP={current_exposure}")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        now = datetime.now()
        timestamp_str = now.strftime('%Y-%m-%d %I:%M:%S %p')

        if (now - last_exposure_update) > timedelta(minutes=1):
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            avg_brightness = np.mean(gray)
            last_brightness = avg_brightness

            if avg_brightness > 150 and exposure_mode != "auto":
                cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.75)
                exposure_mode = "auto"
                print(f"ğŸŒ è‡ªåŠ¨æ›å…‰ï¼ˆäº®åº¦={avg_brightness:.1f}ï¼‰")

            elif avg_brightness < 50 and exposure_mode != "manual":
                cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25)
                exposure_mode = "manual"
                print(f"ğŸŒ™ æ‰‹åŠ¨æ›å…‰ï¼ˆäº®åº¦={avg_brightness:.1f}ï¼‰ â†’ å¼€å§‹è°ƒæ•´")
                adjust_manual_exposure()

            elif exposure_mode == "manual" and current_exposure > EXPOSURE_RANGE[0] and current_exposure < EXPOSURE_RANGE[1]:
                print(f"ğŸ” æ‰‹åŠ¨æ›å…‰ä¿æŒæ¨¡å¼ï¼Œé‡æ–°å°è¯•è®¾ç½® EXP={current_exposure}")
                adjust_manual_exposure()

            last_exposure_update = now

        if now - segment_start_time >= timedelta(hours=SEGMENT_HOURS):
            out.release()
            out, segment_start_time, current_video_path = new_writer()
            print("ğŸ” æ–°å½•åƒï¼š", current_video_path)

        overlay = f"{timestamp_str}  L:{last_brightness:.1f} EXP:{current_exposure}"
        cv2.putText(frame, overlay, (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        out.write(frame)

        if frame_count % SKIP_FRAMES == 0 and frame_queue.qsize() < 10:
            frame_queue.put((frame.copy(), timestamp_str))

        cv2.imshow("Live", frame)
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q') or key == 27:  # q or ESC
            print("ğŸŸ¥ ç”¨æˆ·ä¸»åŠ¨æŒ‰ä¸‹ q æˆ– ESCï¼Œå‡†å¤‡é€€å‡º")
            break

        frame_count += 1

    print("[ä¸»è¿›ç¨‹] æ­£åœ¨é€šçŸ¥å­è¿›ç¨‹é€€å‡º...")
    frame_queue.put("QUIT")
    detector.join(timeout=10)
    print("[ä¸»è¿›ç¨‹] å­è¿›ç¨‹å·²é€€å‡º")

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    print("âœ… ç¨‹åºå·²ä¼˜é›…ç»“æŸï¼Œè§†é¢‘ä¿å­˜è‡³ï¼š", current_video_path)

# ---------------------------
# å¯åŠ¨ä¸»ç¨‹åº
# ---------------------------
if __name__ == '__main__':
    recorder_main()

